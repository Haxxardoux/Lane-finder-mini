{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow ui --port 6007 --backend-store-uri file:/share/lazy/will/ConstrastiveLoss/Logs\n",
    "# watch -n 0.5 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import mlflow\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Quantize(nn.Module):\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_embed = n_embed\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "        flatten = input.reshape(-1, self.dim)\n",
    "        dist = (\n",
    "            flatten.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * flatten @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            embed_onehot_sum = embed_onehot.sum(0)\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "\n",
    "            torch.sum(embed_onehot_sum)\n",
    "            torch.sum(embed_sum)\n",
    "\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                embed_onehot_sum, alpha=1 - self.decay\n",
    "            )\n",
    "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        diff = (quantize.detach() - input).pow(2).mean()\n",
    "        quantize = input + (quantize - input).detach()\n",
    "\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride=1, kernel_size=3, extra_layers=1, residual=True):\n",
    "        super().__init__()\n",
    "        self.residual=residual\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(in_channel, out_channel, stride=stride, kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True)]\n",
    "        \n",
    "        extra_block = [\n",
    "            nn.Conv2d(out_channel, out_channel, stride=1, kernel_size=3, padding=(3-1)//2),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True)]\n",
    "\n",
    "        layers.extend(extra_block)\n",
    "\n",
    "        self.resblock = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return x+self.resblock(x)\n",
    "        else:\n",
    "            return self.resblock(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, channel, extra_layers, stride, kernel_size, residual, extra_residual_blocks, downsample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = channel\n",
    "\n",
    "        blocks = [\n",
    "            ResBlock(in_channel, channel, extra_layers=extra_layers, stride=stride, residual=residual),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "\n",
    "        for i in range(extra_residual_blocks):\n",
    "            blocks.append(ResBlock(in_channel=channel, out_channel=channel, extra_layers=extra_layers, residual=True))\n",
    "            if (downsample=='Once') & (i==0):\n",
    "                blocks.append(nn.MaxPool2d(2, 2))\n",
    "            if (downsample=='Twice') & ((i==0) | (i==1)):\n",
    "                blocks.append(nn.MaxPool2d(2, 2))\n",
    "\n",
    "        self.encode = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.encode(input)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channel, out_channel, extra_layers, extra_residual_blocks, upsample):\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        for i in range(extra_residual_blocks):\n",
    "            blocks.append(ResBlock(in_channel=channel, out_channel=channel, extra_layers=extra_layers, residual=True))\n",
    "            if (upsample=='Twice') & (i==0):\n",
    "                blocks.append(nn.ConvTranspose2d(channel, channel, 2, 2))\n",
    "                            \n",
    "        blocks.append(nn.ConvTranspose2d(channel, out_channel, 2, 2))\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.blocks(input)\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel=3,\n",
    "        channel=128,\n",
    "        n_res_block=2,\n",
    "        n_res_channel=32,\n",
    "        embed_dim=64,\n",
    "        n_embed=512,\n",
    "        decay=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Encoders, first one should have two rounds of downsampling, second should have one\n",
    "        self.enc_b = Encoder(in_channel=in_channel, channel=channel, extra_layers=2, stride=2, kernel_size=5, residual=False, extra_residual_blocks=2, downsample='Once')\n",
    "        self.enc_t = Encoder(in_channel=channel, channel=channel, extra_layers=3, stride=1, kernel_size=3, residual=False, extra_residual_blocks=2, downsample='Once')\n",
    "\n",
    "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
    "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
    "\n",
    "        # Decoders,\n",
    "#         self.dec_t = Decoder(embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2)\n",
    "        self.dec_t = Decoder(embed_dim, embed_dim, channel, extra_residual_blocks = n_res_block, upsample='Once')\n",
    "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
    "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
    "        self.upsample_t = nn.ConvTranspose2d(embed_dim, embed_dim, 4, stride=2, padding=1)\n",
    "#         self.dec = Decoder(embed_dim + embed_dim, in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
    "        self.dec = Decoder(embed_dim + embed_dim, in_channel, extra_layers=2, extra_residual_blocks=2, upsample='Twice')\n",
    "\n",
    "    def forward(self, input):\n",
    "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec, diff\n",
    "\n",
    "    def encode(self, input):\n",
    "        enc_b = self.enc_b(input)\n",
    "        enc_t = self.enc_t(enc_b)\n",
    "\n",
    "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
    "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        diff_t = diff_t.unsqueeze(0)\n",
    "\n",
    "        dec_t = self.dec_t(quant_t)\n",
    "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
    "\n",
    "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
    "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "        diff_b = diff_b.unsqueeze(0)\n",
    "\n",
    "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
    "\n",
    "    def decode(self, quant_t, quant_b):\n",
    "        upsample_t = self.upsample_t(quant_t)\n",
    "        quant = torch.cat([upsample_t, quant_b], 1)\n",
    "        dec = self.dec(quant)\n",
    "\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_t, code_b):\n",
    "        quant_t = self.quantize_t.embed_code(code_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        quant_b = self.quantize_b.embed_code(code_b)\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we add once!\n",
      "we add once!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VQVAE().to('cuda:0')\n",
    "# summary(model, (3, 256, 256))\n",
    "model(torch.ones(1,3,256,256).to('cuda:0'))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing1 = Encoder(in_channel=3, channel=64, extra_layers=2, stride=2, kernel_size=5, residual=False, extra_residual_blocks=2, downsample='Once')\n",
    "thing2 = Encoder(in_channel=thing1.out_channels, channel=64, extra_layers=3, stride=1, kernel_size=3, residual=False, extra_residual_blocks=2, downsample='Once')\n",
    "thing3 = Decoder(64, 32, extra_layers=2, extra_residual_blocks=2, upsample='Twice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "            Conv2d-4         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 128, 128]             128\n",
      "              ReLU-6         [-1, 64, 128, 128]               0\n",
      "          ResBlock-7         [-1, 64, 128, 128]               0\n",
      "              ReLU-8         [-1, 64, 128, 128]               0\n",
      "            Conv2d-9         [-1, 64, 128, 128]          36,928\n",
      "      BatchNorm2d-10         [-1, 64, 128, 128]             128\n",
      "             ReLU-11         [-1, 64, 128, 128]               0\n",
      "           Conv2d-12         [-1, 64, 128, 128]          36,928\n",
      "      BatchNorm2d-13         [-1, 64, 128, 128]             128\n",
      "             ReLU-14         [-1, 64, 128, 128]               0\n",
      "         ResBlock-15         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-16           [-1, 64, 64, 64]               0\n",
      "           Conv2d-17           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-18           [-1, 64, 64, 64]             128\n",
      "             ReLU-19           [-1, 64, 64, 64]               0\n",
      "           Conv2d-20           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-21           [-1, 64, 64, 64]             128\n",
      "             ReLU-22           [-1, 64, 64, 64]               0\n",
      "         ResBlock-23           [-1, 64, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 187,200\n",
      "Trainable params: 187,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 136.00\n",
      "Params size (MB): 0.71\n",
      "Estimated Total Size (MB): 137.46\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]          36,928\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 64, 64]             128\n",
      "              ReLU-6           [-1, 64, 64, 64]               0\n",
      "          ResBlock-7           [-1, 64, 64, 64]               0\n",
      "              ReLU-8           [-1, 64, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-10           [-1, 64, 64, 64]             128\n",
      "             ReLU-11           [-1, 64, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "         ResBlock-15           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17           [-1, 64, 32, 32]          36,928\n",
      "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
      "             ReLU-19           [-1, 64, 32, 32]               0\n",
      "           Conv2d-20           [-1, 64, 32, 32]          36,928\n",
      "      BatchNorm2d-21           [-1, 64, 32, 32]             128\n",
      "             ReLU-22           [-1, 64, 32, 32]               0\n",
      "         ResBlock-23           [-1, 64, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 222,336\n",
      "Trainable params: 222,336\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 34.00\n",
      "Params size (MB): 0.85\n",
      "Estimated Total Size (MB): 35.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(thing1.to('cuda:0'), (3, 256, 256))\n",
    "summary(thing2.to('cuda:0'), (thing1.out_channels, 64, 64))\n",
    "# summary(thing3.to('cuda:0'), (64, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "\n",
    "from VQVAE import VQVAE\n",
    "import distributed as dist\n",
    "\n",
    "class Params(object):\n",
    "    def __init__(self, batch_size, epochs, lr, size):\n",
    "        self.size = batch_size\n",
    "        self.epoch = epochs\n",
    "        self.lr = lr\n",
    "        self.size = size\n",
    "\n",
    "args = Params(64, 1000, 4e-4, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loader, model, optimizer, device):\n",
    "    if dist.is_primary():\n",
    "        loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    latent_loss_weight = 0.25\n",
    "    sample_size = 25\n",
    "\n",
    "    mse_sum = 0\n",
    "    mse_n = 0\n",
    "\n",
    "    for i, (img, labels) in enumerate(loader):\n",
    "        print(i)\n",
    "        model.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "\n",
    "        out, latent_loss = model(img)\n",
    "        recon_loss = criterion(out, img)\n",
    "        latent_loss = latent_loss.mean()\n",
    "        loss = recon_loss + latent_loss_weight * latent_loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        mse_sum += recon_loss.item() * img.shape[0]\n",
    "        mse_n += img.shape[0]\n",
    "\n",
    "        if dist.is_primary():\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            loader.set_description(\n",
    "                (\n",
    "                    f\"epoch: {epoch + 1}; mse: {recon_loss.item():.5f}; \"\n",
    "                    f\"latent: {latent_loss.item():.3f}; avg mse: {mse_sum / mse_n:.5f}; \"\n",
    "                    f\"lr: {lr:.5f}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            sample = img[:sample_size]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out, _ = model(sample)\n",
    "\n",
    "            utils.save_image(\n",
    "                torch.cat([sample, out], 0),\n",
    "                f\"samples/{str(epoch + 1).zfill(5)}_{str(i).zfill(5)}.jpg\",\n",
    "                nrow=sample_size,\n",
    "                normalize=True,\n",
    "                range=(-1, 1),\n",
    "            )\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        yield {'Latent Loss':latent_loss.item(), 'Average MSE':mse_sum/mse_n, 'Reconstruction Loss':recon_loss.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/456 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on epoch number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch: 1; mse: 0.26948; latent: 1.295; avg mse: 0.26948; lr: 0.00040:   0%|          | 0/456 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch: 1; mse: 0.26948; latent: 1.295; avg mse: 0.26948; lr: 0.00040:   0%|          | 1/456 [00:04<32:04,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Loss 1.2951692342758179\n",
      "Average MSE 0.26947730779647827\n",
      "Reconstruction Loss 0.26947730779647827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch: 1; mse: 0.24401; latent: 0.001; avg mse: 0.25674; lr: 0.00040:   0%|          | 1/456 [00:10<32:04,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Latent Loss 0.0010279725538566709\n",
      "Average MSE 0.2567427307367325\n",
      "Reconstruction Loss 0.2440081536769867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1; mse: 0.23317; latent: 0.001; avg mse: 0.24888; lr: 0.00040:   0%|          | 2/456 [00:16<36:05,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Latent Loss 0.0013551212614402175\n",
      "Average MSE 0.24888442953427634\n",
      "Reconstruction Loss 0.233167827129364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1; mse: 0.23317; latent: 0.001; avg mse: 0.24888; lr: 0.00040:   1%|          | 3/456 [00:19<48:46,  6.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-dacf410577d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'We are on epoch number '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-7f80e18b412f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, loader, model, optimizer, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmse_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[1;32m    134\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "mlflow.tracking.set_tracking_uri('file:/share/lazy/will/ConstrastiveLoss/Logs')\n",
    "\n",
    "mlflow.set_experiment('Vector Quantized Variational Autoencoder')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(args.size),\n",
    "        transforms.CenterCrop(args.size),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "dataset = datasets.ImageFolder('/share/lazy/will/ConstrastiveLoss/Imgs/color_images/train/', transform=transform)\n",
    "# sampler = dist.data_sampler(dataset, shuffle=True, distributed=False)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=False, pin_memory = True)\n",
    "\n",
    "model = VQVAE().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "run_name = 'VQVAE 100k images'\n",
    "with mlflow.start_run(run_name = run_name) as run:\n",
    "\n",
    "    for key, value in vars(args).items():\n",
    "        mlflow.log_param(key, value)\n",
    "\n",
    "    mlflow.log_param('Parameters', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        print('We are on epoch number '+str(epoch))\n",
    "        results = train(epoch, loader, model, optimizer, device)\n",
    "        for Dict in results:\n",
    "            for key, value in Dict.items():\n",
    "                print(key, value)\n",
    "                mlflow.log_metric(key, value, epoch)\n",
    "\n",
    "            torch.save({\n",
    "    'model':model.state_dict(),\n",
    "    'optimizer':optimizer.state_dict(),\n",
    "    }, 'run_stats.pyt')\n",
    "    mlflow.log_artifact('run_stats.pyt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributed as dist\n",
    "def get_world_size():\n",
    "    if not torch.distributed.dist.is_available():\n",
    "        return 1\n",
    "\n",
    "    if not torch.distributed.dist.is_initialized():\n",
    "        return 1\n",
    "\n",
    "    return torch.distributed.dist.get_world_size()\n",
    "\n",
    "\n",
    "def all_reduce(tensor, op=dist.ReduceOp.SUM):\n",
    "    world_size = get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return tensor\n",
    "\n",
    "    dist.all_reduce(tensor, op=op)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Quantize(nn.Module):\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_embed = n_embed\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "        flatten = input.reshape(-1, self.dim)\n",
    "        dist = (\n",
    "            flatten.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * flatten @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            embed_onehot_sum = embed_onehot.sum(0)\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "\n",
    "            torch.sum(embed_onehot_sum)\n",
    "            torch.sum(embed_sum)\n",
    "\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                embed_onehot_sum, alpha=1 - self.decay\n",
    "            )\n",
    "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        diff = (quantize.detach() - input).pow(2).mean()\n",
    "        quantize = input + (quantize - input).detach()\n",
    "\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channel, channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel, in_channel, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "        out += input\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        if stride == 4:\n",
    "            blocks = [\n",
    "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            ]\n",
    "\n",
    "        elif stride == 2:\n",
    "            blocks = [\n",
    "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 2, channel, 3, padding=1),\n",
    "            ]\n",
    "\n",
    "        for i in range(n_res_block):\n",
    "            blocks.append(ResBlock(channel, n_res_channel))\n",
    "\n",
    "        blocks.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.blocks(input)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channel, out_channel, channel, n_res_block, n_res_channel, stride\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = [nn.Conv2d(in_channel, channel, 3, padding=1)]\n",
    "\n",
    "        for i in range(n_res_block):\n",
    "            blocks.append(ResBlock(channel, n_res_channel))\n",
    "\n",
    "        blocks.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        if stride == 4:\n",
    "            blocks.extend(\n",
    "                [\n",
    "                    nn.ConvTranspose2d(channel, channel // 2, 4, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.ConvTranspose2d(\n",
    "                        channel // 2, out_channel, 4, stride=2, padding=1\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif stride == 2:\n",
    "            blocks.append(\n",
    "                nn.ConvTranspose2d(channel, out_channel, 4, stride=2, padding=1)\n",
    "            )\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.blocks(input)\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel=3,\n",
    "        channel=128,\n",
    "        n_res_block=2,\n",
    "        n_res_channel=32,\n",
    "        embed_dim=64,\n",
    "        n_embed=512,\n",
    "        decay=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
    "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
    "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
    "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
    "        self.dec_t = Decoder(\n",
    "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
    "        )\n",
    "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
    "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
    "        self.upsample_t = nn.ConvTranspose2d(\n",
    "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
    "        )\n",
    "        self.dec = Decoder(\n",
    "            embed_dim + embed_dim,\n",
    "            in_channel,\n",
    "            channel,\n",
    "            n_res_block,\n",
    "            n_res_channel,\n",
    "            stride=4,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec, diff\n",
    "\n",
    "    def encode(self, input):\n",
    "        enc_b = self.enc_b(input)\n",
    "        enc_t = self.enc_t(enc_b)\n",
    "\n",
    "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
    "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        diff_t = diff_t.unsqueeze(0)\n",
    "\n",
    "        dec_t = self.dec_t(quant_t)\n",
    "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
    "\n",
    "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
    "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "        diff_b = diff_b.unsqueeze(0)\n",
    "\n",
    "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
    "\n",
    "    def decode(self, quant_t, quant_b):\n",
    "        upsample_t = self.upsample_t(quant_t)\n",
    "        quant = torch.cat([upsample_t, quant_b], 1)\n",
    "        dec = self.dec(quant)\n",
    "\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_t, code_b):\n",
    "        quant_t = self.quantize_t.embed_code(code_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        quant_b = self.quantize_b.embed_code(code_b)\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "              ReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
      "              ReLU-4          [-1, 128, 64, 64]               0\n",
      "            Conv2d-5          [-1, 128, 64, 64]         147,584\n",
      "              ReLU-6          [-1, 128, 64, 64]               0\n",
      "            Conv2d-7           [-1, 32, 64, 64]          36,896\n",
      "              ReLU-8           [-1, 32, 64, 64]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]           4,224\n",
      "         ResBlock-10          [-1, 128, 64, 64]               0\n",
      "             ReLU-11          [-1, 128, 64, 64]               0\n",
      "           Conv2d-12           [-1, 32, 64, 64]          36,896\n",
      "             ReLU-13           [-1, 32, 64, 64]               0\n",
      "           Conv2d-14          [-1, 128, 64, 64]           4,224\n",
      "         ResBlock-15          [-1, 128, 64, 64]               0\n",
      "             ReLU-16          [-1, 128, 64, 64]               0\n",
      "          Encoder-17          [-1, 128, 64, 64]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]         131,136\n",
      "             ReLU-19           [-1, 64, 32, 32]               0\n",
      "           Conv2d-20          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22           [-1, 32, 32, 32]          36,896\n",
      "             ReLU-23           [-1, 32, 32, 32]               0\n",
      "           Conv2d-24          [-1, 128, 32, 32]           4,224\n",
      "         ResBlock-25          [-1, 128, 32, 32]               0\n",
      "             ReLU-26          [-1, 128, 32, 32]               0\n",
      "           Conv2d-27           [-1, 32, 32, 32]          36,896\n",
      "             ReLU-28           [-1, 32, 32, 32]               0\n",
      "           Conv2d-29          [-1, 128, 32, 32]           4,224\n",
      "         ResBlock-30          [-1, 128, 32, 32]               0\n",
      "             ReLU-31          [-1, 128, 32, 32]               0\n",
      "          Encoder-32          [-1, 128, 32, 32]               0\n",
      "           Conv2d-33           [-1, 64, 32, 32]           8,256\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-877627c5c814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVQVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m         \u001b[0mtotal_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nb_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtotal_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"trainable\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trainable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2909\u001b[0m     \"\"\"\n\u001b[1;32m   2910\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 2911\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/miniconda3/envs/june2020-gpu/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "model = VQVAE().to('cuda:0')\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "june2020-gpu",
   "language": "python",
   "name": "june2020-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
